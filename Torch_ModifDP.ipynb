{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "Common\n",
      "E:/1st Project/Class/Common/2100/\n",
      "1\n",
      "Size of the original feature is: 10000 x 98.\n",
      "Size of the knockoff feature is: 10000 x 98.\n",
      "Size of the target is: 10000 x 1.\n",
      "Size of the output weight is: 98 x 1.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import BCELoss, Linear, Module, ReLU, Sigmoid\n",
    "from torch.nn.init import kaiming_uniform_, xavier_uniform_\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "################################################################################\n",
    "ML_Type = 'Class'\n",
    "Gene_Type = 'Common'\n",
    "Feature_Size = 2100  # Feature size {50, 100, 200, 400, 600, 800, 1000}\n",
    "print(ML_Type)\n",
    "print(Gene_Type)\n",
    "#print(\"Size of the Columns/Features is: %d.\" %(Feature_Size))\n",
    "dataDir = 'E:/1st Project/'\n",
    "dataDir = dataDir + str(ML_Type) + '/' + str(Gene_Type) + '/' + str(Feature_Size) + '/'\n",
    "outputDir = dataDir\n",
    "dataType = '';\n",
    "print(dataDir)\n",
    "################################################################################\n",
    "indx = 1\n",
    "print(indx)\n",
    "prt1 = 'X_orig_'+str(indx)+'.csv'\n",
    "prt2 = 'X_ko1_'+str(indx)+'.csv'; \n",
    "prt7 = 'Y_'+str(indx)+'.csv'; \n",
    "prt8 = 'Beta_'+str(indx)+'.csv';\n",
    "\n",
    "output_path1 = dataDir + prt1\n",
    "output_path2 = dataDir + prt2; \n",
    "output_path7 = dataDir + prt7; \n",
    "output_path8 = dataDir + prt8\n",
    "\n",
    "X_orig = pd.read_csv(output_path1, header = None).values.astype(np.float32);\n",
    "X_ko1 = pd.read_csv(output_path2, header = None).values.astype(np.float32);\n",
    "Y = pd.read_csv(output_path7, header = None).values.astype(np.float32);\n",
    "Beta = pd.read_csv(output_path8, header = None).values.astype(np.float32);\n",
    "\n",
    "print(\"Size of the original feature is: %d x %d.\" %(X_orig.shape))\n",
    "print(\"Size of the knockoff feature is: %d x %d.\" %(X_ko1.shape))\n",
    "print(\"Size of the target is: %d x %d.\" %(Y.shape))\n",
    "print(\"Size of the output weight is: %d x %d.\" %(Beta.shape))\n",
    "Num_knock = 1;\n",
    "bias = True;\n",
    "num_ins = X_orig.shape[0];\n",
    "num_dim = X_orig.shape[1];\n",
    "num_epochs = 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all2 = np.zeros((num_ins,num_dim*2), dtype=float)\n",
    "for i in range(num_dim):\n",
    "    temp_o = X_orig[:,i].reshape((num_ins,1))\n",
    "    temp_k = X_ko1[:,i].reshape((num_ins,1))\n",
    "    X_all2[:,2*i:2*(i+1)] = np.concatenate((temp_o,temp_k), axis=1)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_all2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_net.py\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "Y = torch.from_numpy(Y)\n",
    "X_all2 = torch.from_numpy(X_all2).float()\n",
    "X_all2.requires_grad=True\n",
    "type(X_all2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_channels: The size of the new dimension is called the number of output channels, or number of output feature maps\n",
    "# In case of an RGB image, the size of the new dimension (aside from the width/height of the image) is 3.\n",
    "#This \"size of the 3rd dimension\" is called the number of input channels or number of input feature maps.\n",
    "#In the above image example, the number of input channels is 3, and we have a 3×3×3 kernel.\n",
    "\n",
    "import torch.nn as nn\n",
    "class LocallyConnected1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, output_size, kernel_size, stride, bias=False):\n",
    "        super(LocallyConnected1d, self).__init__()\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(1, out_channels, in_channels, output_size, kernel_size)\n",
    "        )\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(\n",
    "                torch.randn(1, out_channels, output_size)\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #_, c, h = x.size()\n",
    "        c, h = x.size()\n",
    "#        print(c)\n",
    "#        print(h)\n",
    "        kh = self.kernel_size\n",
    "        print(kh)\n",
    "        dh = self.stride\n",
    "        print(dh)\n",
    "        #x = x.unfold(2, kh, dh)\n",
    "        x = x.unfold(1, kh, dh)\n",
    "        print(x.shape)\n",
    "        # x = x.contiguous().view(*x.size()[:-2], -1)\n",
    "        # Sum in in_channel and kernel_size dims\n",
    "        x = x.unsqueeze(1)\n",
    "        out = (x * self.weight)\n",
    "        out = out.sum([2, -1])\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 2048\n",
    "hidden_size = 40\n",
    "learning_rate = 0.01\n",
    "kernel = stride = 2\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, num_dim, hidden_size):\n",
    "        super(NeuralNet, self).__init__()      \n",
    "        self.fc1 = nn.Linear(2*num_dim, hidden_size) \n",
    "        xavier_uniform_(self.fc1.weight)\n",
    "        self.Act1 = torch.nn.functional.elu\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)  \n",
    "        xavier_uniform_(self.fc2.weight)\n",
    "        self.Act2 = torch.nn.functional.elu\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        xavier_uniform_(self.fc3.weight)\n",
    "        self.Act3 = Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.Act1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.Act2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.Act3(out)\n",
    "        return out\n",
    "        \n",
    "    def compute_l1_loss(self, w):\n",
    "        return torch.abs(w).sum()        \n",
    "    \n",
    "\n",
    "Model = NeuralNet(num_dim, hidden_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss() # initialize loss function\n",
    "optimizer = torch.optim.Adam(Model.parameters(), lr=learning_rate)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Epoch [1/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [1/120], Step [2/5], Loss: 0.5050\n",
      "Epoch [1/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [1/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [1/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [2/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [2/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [2/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [2/120], Step [4/5], Loss: 0.4934\n",
      "Epoch [2/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [3/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [3/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [3/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [3/120], Step [4/5], Loss: 0.4934\n",
      "Epoch [3/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [4/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [4/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [4/120], Step [3/5], Loss: 0.4961\n",
      "Epoch [4/120], Step [4/5], Loss: 0.4934\n",
      "Epoch [4/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [5/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [5/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [5/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [5/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [5/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [6/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [6/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [6/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [6/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [6/120], Step [5/5], Loss: 0.5054\n",
      "Epoch [7/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [7/120], Step [2/5], Loss: 0.5056\n",
      "Epoch [7/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [7/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [7/120], Step [5/5], Loss: 0.5054\n",
      "Epoch [8/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [8/120], Step [2/5], Loss: 0.5056\n",
      "Epoch [8/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [8/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [8/120], Step [5/5], Loss: 0.5054\n",
      "Epoch [9/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [9/120], Step [2/5], Loss: 0.5056\n",
      "Epoch [9/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [9/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [9/120], Step [5/5], Loss: 0.5054\n",
      "Epoch [10/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [10/120], Step [2/5], Loss: 0.5056\n",
      "Epoch [10/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [10/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [10/120], Step [5/5], Loss: 0.5054\n",
      "Epoch [11/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [11/120], Step [2/5], Loss: 0.5056\n",
      "Epoch [11/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [11/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [11/120], Step [5/5], Loss: 0.5054\n",
      "Epoch [12/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [12/120], Step [2/5], Loss: 0.5056\n",
      "Epoch [12/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [12/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [12/120], Step [5/5], Loss: 0.5054\n",
      "Epoch [13/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [13/120], Step [2/5], Loss: 0.5056\n",
      "Epoch [13/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [13/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [13/120], Step [5/5], Loss: 0.5054\n",
      "Epoch [14/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [14/120], Step [2/5], Loss: 0.5056\n",
      "Epoch [14/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [14/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [14/120], Step [5/5], Loss: 0.5054\n",
      "Epoch [15/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [15/120], Step [2/5], Loss: 0.5056\n",
      "Epoch [15/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [15/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [15/120], Step [5/5], Loss: 0.5054\n",
      "Epoch [16/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [16/120], Step [2/5], Loss: 0.5056\n",
      "Epoch [16/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [16/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [16/120], Step [5/5], Loss: 0.5054\n",
      "Epoch [17/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [17/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [17/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [17/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [17/120], Step [5/5], Loss: 0.5054\n",
      "Epoch [18/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [18/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [18/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [18/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [18/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [19/120], Step [1/5], Loss: 0.4981\n",
      "Epoch [19/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [19/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [19/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [19/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [20/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [20/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [20/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [20/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [20/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [21/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [21/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [21/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [21/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [21/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [22/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [22/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [22/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [22/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [22/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [23/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [23/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [23/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [23/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [23/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [24/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [24/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [24/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [24/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [24/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [25/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [25/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [25/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [25/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [25/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [26/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [26/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [26/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [26/120], Step [4/5], Loss: 0.4933\n",
      "Epoch [26/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [27/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [27/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [27/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [27/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [27/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [28/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [28/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [28/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [28/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [28/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [29/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [29/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [29/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [29/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [29/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [30/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [30/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [30/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [30/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [30/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [31/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [31/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [31/120], Step [3/5], Loss: 0.4960\n",
      "Epoch [31/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [31/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [32/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [32/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [32/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [32/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [32/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [33/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [33/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [33/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [33/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [33/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [34/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [34/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [34/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [34/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [34/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [35/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [35/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [35/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [35/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [35/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [36/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [36/120], Step [2/5], Loss: 0.5055\n",
      "Epoch [36/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [36/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [36/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [37/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [37/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [37/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [37/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [37/120], Step [5/5], Loss: 0.5053\n",
      "Epoch [38/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [38/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [38/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [38/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [38/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [39/120], Step [1/5], Loss: 0.4980\n",
      "Epoch [39/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [39/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [39/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [39/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [40/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [40/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [40/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [40/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [40/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [41/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [41/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [41/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [41/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [41/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [42/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [42/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [42/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [42/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [42/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [43/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [43/120], Step [2/5], Loss: 0.5054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [43/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [43/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [44/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [44/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [44/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [44/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [44/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [45/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [45/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [45/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [45/120], Step [4/5], Loss: 0.4932\n",
      "Epoch [45/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [46/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [46/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [46/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [46/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [46/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [47/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [47/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [47/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [47/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [47/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [48/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [48/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [48/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [48/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [48/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [49/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [49/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [49/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [49/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [49/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [50/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [50/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [50/120], Step [3/5], Loss: 0.4959\n",
      "Epoch [50/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [50/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [51/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [51/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [51/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [51/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [51/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [52/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [52/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [52/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [52/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [52/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [53/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [53/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [53/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [53/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [53/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [54/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [54/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [54/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [54/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [54/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [55/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [55/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [55/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [55/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [55/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [56/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [56/120], Step [2/5], Loss: 0.5054\n",
      "Epoch [56/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [56/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [56/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [57/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [57/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [57/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [57/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [57/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [58/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [58/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [58/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [58/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [58/120], Step [5/5], Loss: 0.5052\n",
      "Epoch [59/120], Step [1/5], Loss: 0.4979\n",
      "Epoch [59/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [59/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [59/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [59/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [60/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [60/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [60/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [60/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [60/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [61/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [61/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [61/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [61/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [61/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [62/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [62/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [62/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [62/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [62/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [63/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [63/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [63/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [63/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [63/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [64/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [64/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [64/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [64/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [64/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [65/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [65/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [65/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [65/120], Step [4/5], Loss: 0.4931\n",
      "Epoch [65/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [66/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [66/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [66/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [66/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [66/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [67/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [67/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [67/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [67/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [67/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [68/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [68/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [68/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [68/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [68/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [69/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [69/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [69/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [69/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [69/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [70/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [70/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [70/120], Step [3/5], Loss: 0.4958\n",
      "Epoch [70/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [70/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [71/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [71/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [71/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [71/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [71/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [72/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [72/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [72/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [72/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [72/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [73/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [73/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [73/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [73/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [73/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [74/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [74/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [74/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [74/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [74/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [75/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [75/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [75/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [75/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [75/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [76/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [76/120], Step [2/5], Loss: 0.5053\n",
      "Epoch [76/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [76/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [76/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [77/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [77/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [77/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [77/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [77/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [78/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [78/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [78/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [78/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [78/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [79/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [79/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [79/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [79/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [79/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [80/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [80/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [80/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [80/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [80/120], Step [5/5], Loss: 0.5051\n",
      "Epoch [81/120], Step [1/5], Loss: 0.4978\n",
      "Epoch [81/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [81/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [81/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [81/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [82/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [82/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [82/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [82/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [82/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [83/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [83/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [83/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [83/120], Step [4/5], Loss: 0.4930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [84/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [84/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [84/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [84/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [84/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [85/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [85/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [85/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [85/120], Step [4/5], Loss: 0.4930\n",
      "Epoch [85/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [86/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [86/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [86/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [86/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [86/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [87/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [87/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [87/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [87/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [87/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [88/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [88/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [88/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [88/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [88/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [89/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [89/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [89/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [89/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [89/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [90/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [90/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [90/120], Step [3/5], Loss: 0.4957\n",
      "Epoch [90/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [90/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [91/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [91/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [91/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [91/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [91/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [92/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [92/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [92/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [92/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [92/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [93/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [93/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [93/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [93/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [93/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [94/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [94/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [94/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [94/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [94/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [95/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [95/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [95/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [95/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [95/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [96/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [96/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [96/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [96/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [96/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [97/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [97/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [97/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [97/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [97/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [98/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [98/120], Step [2/5], Loss: 0.5052\n",
      "Epoch [98/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [98/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [98/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [99/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [99/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [99/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [99/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [99/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [100/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [100/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [100/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [100/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [100/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [101/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [101/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [101/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [101/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [101/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [102/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [102/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [102/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [102/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [102/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [103/120], Step [1/5], Loss: 0.4977\n",
      "Epoch [103/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [103/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [103/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [103/120], Step [5/5], Loss: 0.5050\n",
      "Epoch [104/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [104/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [104/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [104/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [104/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [105/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [105/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [105/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [105/120], Step [4/5], Loss: 0.4929\n",
      "Epoch [105/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [106/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [106/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [106/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [106/120], Step [4/5], Loss: 0.4928\n",
      "Epoch [106/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [107/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [107/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [107/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [107/120], Step [4/5], Loss: 0.4928\n",
      "Epoch [107/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [108/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [108/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [108/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [108/120], Step [4/5], Loss: 0.4928\n",
      "Epoch [108/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [109/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [109/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [109/120], Step [3/5], Loss: 0.4956\n",
      "Epoch [109/120], Step [4/5], Loss: 0.4928\n",
      "Epoch [109/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [110/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [110/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [110/120], Step [3/5], Loss: 0.4955\n",
      "Epoch [110/120], Step [4/5], Loss: 0.4928\n",
      "Epoch [110/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [111/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [111/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [111/120], Step [3/5], Loss: 0.4955\n",
      "Epoch [111/120], Step [4/5], Loss: 0.4928\n",
      "Epoch [111/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [112/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [112/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [112/120], Step [3/5], Loss: 0.4955\n",
      "Epoch [112/120], Step [4/5], Loss: 0.4928\n",
      "Epoch [112/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [113/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [113/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [113/120], Step [3/5], Loss: 0.4955\n",
      "Epoch [113/120], Step [4/5], Loss: 0.4928\n",
      "Epoch [113/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [114/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [114/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [114/120], Step [3/5], Loss: 0.4955\n",
      "Epoch [114/120], Step [4/5], Loss: 0.4928\n",
      "Epoch [114/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [115/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [115/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [115/120], Step [3/5], Loss: 0.4955\n",
      "Epoch [115/120], Step [4/5], Loss: 0.4928\n",
      "Epoch [115/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [116/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [116/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [116/120], Step [3/5], Loss: 0.4955\n",
      "Epoch [116/120], Step [4/5], Loss: 0.4928\n",
      "Epoch [116/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [117/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [117/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [117/120], Step [3/5], Loss: 0.4955\n",
      "Epoch [117/120], Step [4/5], Loss: 0.4928\n",
      "Epoch [117/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [118/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [118/120], Step [2/5], Loss: 0.5051\n",
      "Epoch [118/120], Step [3/5], Loss: 0.4955\n",
      "Epoch [118/120], Step [4/5], Loss: 0.4928\n",
      "Epoch [118/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [119/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [119/120], Step [2/5], Loss: 0.5050\n",
      "Epoch [119/120], Step [3/5], Loss: 0.4955\n",
      "Epoch [119/120], Step [4/5], Loss: 0.4928\n",
      "Epoch [119/120], Step [5/5], Loss: 0.5049\n",
      "Epoch [120/120], Step [1/5], Loss: 0.4976\n",
      "Epoch [120/120], Step [2/5], Loss: 0.5050\n",
      "Epoch [120/120], Step [3/5], Loss: 0.4955\n",
      "Epoch [120/120], Step [4/5], Loss: 0.4928\n",
      "Epoch [120/120], Step [5/5], Loss: 0.5049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8012"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "grad_bank = {}\n",
    "avg_counter = 0\n",
    "num_ins = X_all2.shape[0]\n",
    "num_epochs = 120\n",
    "p_norm = 1\n",
    "l1_lambda = 0.001\n",
    "Batch_counter = int(np.ceil(num_ins/batch_size))\n",
    "total_step = Batch_counter\n",
    "print(Batch_counter)\n",
    "for epoch in range(num_epochs):   \n",
    "    for step in range(Batch_counter):\n",
    "        step1 = (step*batch_size)\n",
    "        if step == (Batch_counter-1):\n",
    "            step2 = num_ins\n",
    "        else: \n",
    "            step2 = (step+1)*batch_size\n",
    "        # Forward pass\n",
    "        X_batch = X_all2[step1:step2,].clone().requires_grad_(True)\n",
    "        outputs = Model(X_batch) \n",
    "        outputs = torch.squeeze(outputs, 0)\n",
    "        loss = criterion(outputs, Y[step1:step2,])  \n",
    "        #regularity =  torch.norm(model.loc1.weight, p=p_norm)\n",
    "        #cost = loss + l1_lambda * regularity\n",
    "        \n",
    "        # Compute L1 loss component\n",
    "        l1_weight = 0.00001\n",
    "        l1_parameters = []\n",
    "        for parameter in Model.parameters():\n",
    "            l1_parameters.append(parameter.view(-1))\n",
    "        l1 = l1_weight * Model.compute_l1_loss(torch.cat(l1_parameters))\n",
    "      \n",
    "        # Add L1 loss component\n",
    "        loss += l1        \n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        #for idx, param in enumerate(model.parameters()):\n",
    "         #   grad_bank[idx] += param.grad.data\n",
    "          #  avg_counter += 1        \n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "               .format(epoch+1, num_epochs, step+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# evaluate the model\n",
    "yhat = Model(X_all2)\n",
    "yhat = yhat.detach().numpy()\n",
    "yhat.shape\n",
    "actual = Y.numpy()\n",
    "actual = actual.reshape((len(actual), 1))\n",
    "# round to class values\n",
    "yhat = yhat.round()\n",
    "yhat = yhat.reshape((num_ins, 1))\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(Y, yhat)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16f1a94f670>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#YY = model.loc1.weight\n",
    "X_all2.grad\n",
    "YY = torch.mean(X_all2.grad,0)\n",
    "YY\n",
    "#max_x=tf.reduce_max(x.detach().numpy());\n",
    "#max_x\n",
    "XX = np.arange(0,YY.shape[0])+1\n",
    "plt.plot(XX, abs(YY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 196])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all2.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.045774e-09>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =  torch.mean(X_all2.grad,0)\n",
    "max_x=tf.reduce_min(abs(x).detach().numpy());\n",
    "max_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "XX = torch.mean(X_all2.grad,0)\n",
    "filename_metric = 'Grad2' + '.csv'\n",
    "with open(os.path.join('E:/', filename_metric), \"a+\") as fp:\n",
    "    wr = csv.writer(fp, dialect='excel'); \n",
    "    wr.writerow(abs(XX));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.Flatten()\n",
    "x = torch.randn((2, 2, 3))\n",
    "print('Before flatten shape : ', x.shape)\n",
    "print('After flatten shape : ', f(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.Tensor(np.ones(shape=[10, 2, 3, 4]))\n",
    "print(input_tensor.shape)\n",
    "flatten1 = torch.nn.Sequential(torch.nn.Flatten(start_dim=1))\n",
    "\n",
    "output1 = flatten1(input_tensor)\n",
    "print(output1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [-0.42280453],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.31364927],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 1.138121  ],\n",
       "       [-1.252744  ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Beta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "759px",
    "left": "1225px",
    "right": "20px",
    "top": "121px",
    "width": "355px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
